{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c203bbd",
   "metadata": {},
   "source": [
    "# Lab 3.2 : Using LangChain with IBM WatsonX\n",
    "\n",
    "## 1. Intro to LangChain\n",
    "\n",
    "[LangChain](https://docs.langchain.com/docs/) is an open-source development framework designed to simplify the creation of applications using large language models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Here are the main components for the LangChain\n",
    "\n",
    "- Model: interact with various LLMs\n",
    "- Prompts: text that is sent to the LLMs\n",
    "- Chains: allow to combine different LLM calls and actions automatically\n",
    "- Embeddings and Vector Stores: break large data into chunks and store those to be queried when relevant\n",
    "- Agents: enbale the LLMs to dynamically decide which tools to use in order to best respond to a given query\n",
    "\n",
    "In short, **Langchain is a framework that can orchestrate a series of prompts to achieve a desired outcomes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c30de",
   "metadata": {},
   "source": [
    "## 2. How to connect LangChain to WatsonX.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundation Model at Watsonx.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' You can call the model using their path'''\n",
    "mt_model = \"bigscience/mt0-xxl\"\n",
    "llama2= \"meta-llama/llama-2-70b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4adcdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, List, Mapping, Optional, Union, Dict\n",
    "from pydantic import BaseModel, Extra\n",
    "try:\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace embedding models\n",
    "    from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "    from langchain.llms.base import LLM\n",
    "    from langchain.llms.utils import enforce_stop_tokens\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import langchain: Please install ibm-generative-ai[langchain] extension.\")\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config Watsonx.ai environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##initializing model's parameters\n",
    "\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 2,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e37376-3f85-4751-9aba-9550af6cbf37",
   "metadata": {},
   "source": [
    "In order to use WatsonX-based LLMs with Langchain, the LLM object must be of class `BaseLanguageModel` (see [Langchain docs](https://api.python.langchain.com/en/latest/schema/langchain.schema.language_model.BaseLanguageModel.html)). We'll use the custom class below to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805b7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the WatsonX Model in a langchain.llms.base.LLM subclass to allow LangChain to interact with the model\n",
    "\n",
    "class LangChainInterface(LLM, BaseModel):\n",
    "    credentials: Optional[Dict] = None\n",
    "    model: Optional[str] = None\n",
    "    params: Optional[Dict] = None\n",
    "    project_id : Optional[str]=None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        _params = self.params or {}\n",
    "        return {\n",
    "            **{\"model\": self.model},\n",
    "            **{\"params\": _params},\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"IBM WATSONX\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the WatsonX model\"\"\"\n",
    "        params = self.params or {}\n",
    "        model = Model(model_id=self.model, params=params, credentials=self.credentials, project_id=self.project_id)\n",
    "        text = model.generate_text(prompt)\n",
    "        if stop is not None:\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bigscience/mt0-xxl memberikan hasil:\n",
      "Seoul\n",
      "Model meta-llama/llama-2-70b-chat memberikan hasil:\n",
      "\n",
      "Where is the capital of South Korea?\n",
      "The capital of South Korea is Seoul. Se\n"
     ]
    }
   ],
   "source": [
    "##predict with the model\n",
    "''' Pada Class LangChainInterface kita telah menambahkan fungsi wrapper untuk menjalankan model kita, sehingga kita cukup memanggil model_id yang telah kita \n",
    "definisikan sebelumnya ''' \n",
    "\n",
    "#Lalu kita dapat mencoba mengajukan pertanyaan sederhana dan lihat bagaimana model memberikan respon\n",
    "model_list = [mt_model, llama2]\n",
    "text = \"Dimana ibukota dari Korea Selatan?\"\n",
    "for i in model_list: \n",
    "    llm_model = LangChainInterface(model=i, credentials=creds, params=params, project_id=project_id)\n",
    "    print(f\"\\nModel {i} memberikan hasil:\")\n",
    "    print(llm_model(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2e8f",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates & Chains\n",
    "\n",
    "In the previous example, the user input is sent directly to the LLM. However, when using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios\n",
    "\n",
    "- Accepting user input and contruct a prompt\n",
    "- Generating mutiple prompts from an collection of data points in a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310c2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apa ibukota dari negara USA? = Washington, D.C.\n",
      "Apa ibukota dari negara Inggris? = London\n",
      "Apa ibukota dari negara Jepang? = Tokyo\n",
      "Apa ibukota dari negara Arab Saudi? = Riyadh\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt templates\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"country\"],\n",
    "  template= \"Apa ibukota dari negara {country}?\",\n",
    ")\n",
    "llm_model = LangChainInterface(model=mt_model, credentials=creds, params=params, project_id=project_id)\n",
    "# Chaining \n",
    "chain = LLMChain(llm=llm_model, prompt=prompt)\n",
    "\n",
    "# Getting predictions\n",
    "countries = [\"USA\", \"Inggris\", \"Jepang\", \"Arab Saudi\"]\n",
    "for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a9df3",
   "metadata": {},
   "source": [
    "## 4. Simple sequential chains\n",
    "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
    "\n",
    "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffda7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topik\"], \n",
    "                    template=\"Buat suatu pertanyaan yang berkaitan dengan topik {topik}: Pertanyaan: \")\n",
    "pt2 = PromptTemplate(input_variables=[\"pertanyaan\"],\n",
    "                     template=\"Jawab pertanyaan berikut : {pertanyaan}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23e4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_model = LangChainInterface(model='bigscience/mt0-xxl', credentials=creds, params=params, project_id=project_id)\n",
    "answer_model=  LangChainInterface(model='bigscience/mt0-xxl', credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35de1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_question= LLMChain(llm=question_model, prompt=pt1)\n",
    "question_to_answer = LLMChain(llm=answer_model, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_question, question_to_answer], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34586549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mApa yang dimaksud dengan laptop?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mKomputer portabel\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Komputer portabel'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Laptop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c152d",
   "metadata": {},
   "source": [
    "## 5. Easy Loading of Documents Using Lang Chain\n",
    "LangChain makes it easy to extract passages from documents so that you can answering questions based on your document's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1743ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf='pdfs/Machine_Learning.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9348f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=300\n",
    "    , chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a890f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initializing watsonx bigsciece/mt-model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300,\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.TOP_P:1\n",
    "}\n",
    "\n",
    "model = LangChainInterface(model=\"bigscience/mt0-xxl\", credentials=creds, params=params, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1a50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"refine\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6efed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning dapat didefinisikan sebagai aplikasi komputer dan algoritma matematika yang diadopsi dengan cara pembelajaran yang berasal dari data dan menghasilkan prediksi di masa yang akan datang (Goldberg & Holland, 1988) . Adapun proses pembelajaran yang dimaksud adalah suatu usaha dalam memperoleh kecerdasan yang melalui dua tahap an tara lain latihan ( training ) dan pengujian (testing) (Huang, Zhu, & Siew, 2006) . Machine learning dapat didefinisikan sebagai aplikasi komputer dan algoritma matematika yang diadopsi dengan cara pembelajaran yang berasal dari data dan menghasilkan prediksi di masa yang akan datang (Goldberg & Holland, 1988) . Adapun proses pembelajaran yang dimaksud adalah suatu usaha dalam memperoleh kecerdasan yang melalui dua tahap an tara lain latihan ( training ) dan pengujian (testing) (Huang, Zhu, & Siew, 2006) . Machine learning dapat didefinisikan sebagai aplikasi komputer dan algoritma matematika yang diadopsi dengan cara pembelajaran yang berasal dari data dan menghasilkan prediksi di masa yang akan datang (Goldberg & Holland, 1988) . Adapun proses pembelajaran yang dimaksud adalah suatu usaha dalam memperoleh kecerdasan yang melalui dua tahap an tara lain latihan ( training ) dan pengujian (testing) (H'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##answering based on the documents \n",
    "chain.run(\"Apa yang dimaksud dengan Machine Learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model dapat mengeluarkan hasil dengan melakukan pencarian terhadap dokumen yang diinginkan, namun untuk bisa meningkatkan kualitas dari output, silakan coba untuk melakukan parameter tuning ataupun mencoba model lain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional: Prompt Instruction in Solving Problem Using Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of All we will playing with the prompt template\n",
    "question_template = '''\n",
    "<s>[INST] <<SYS>>\n",
    "INSTRUCTION:\n",
    "Kamu adalah seseorang yang memiliki ketertarikan dengan tempat wisata yang ada di dunia. \n",
    "Tanyakan suatu pertanyaan terkait tempat wisata yang diajukan pada bagian 'TOPIK'.\n",
    "Pertanyaan tersebut harus menggunakan bahasa Indonesia yang sesuai dengan PUEBI\n",
    "Pastikan pertanyaan Hanya satu kalimat. Gunakan Contoh dibawah ini:\n",
    "Topik: Jepang\n",
    "Pertanyaan: Gunung yang terkenal di Jepang?\n",
    "<</SYS>>\n",
    "INPUT:\n",
    "TOPIK:{topik}\n",
    "[/INST] \n",
    "'''\n",
    "\n",
    "\n",
    "answer_template = '''\n",
    "<s>[INST] <<SYS>>\n",
    "Kamu adalah seorang expert tempat wisata yang ada di dunia yang sudah pernah mengunjungi berbagai tempat di dunia. Akan ada pertanyaan untukmu terkait dengan tempat wisata.\n",
    "Hanya langsung berikan jawaban yang diharapkan.\n",
    "Gunakan contoh di bawah ini:\n",
    "Pertanyaan: Gunung yang terkenal di Jepang?\n",
    "Jawaban: Fuji\n",
    "Pertanyaan: Pegunungan tertinggi di Dunia?\n",
    "Jawaban: Himalaya\n",
    "<</SYS>>\n",
    "Pertanyaan: {pertanyaan}\n",
    "[/INST] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"topik\"], \n",
    "    template=question_template\n",
    ")\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"pertanyaan\"],\n",
    "    template=answer_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mPertanyaan: Tempat wisata alam yang terkenal di Indonesia?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mJawaban: Taman Nasional Gunung Gede Pangrango\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jawaban: Taman Nasional Gunung Gede Pangrango'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20,\n",
    "    GenParams.STOP_SEQUENCES: ['\\n']\n",
    "}\n",
    "\n",
    "question_model = LangChainInterface(model=llama2, credentials=creds, params=params, project_id=project_id)\n",
    "answer_model=  LangChainInterface(model=llama2, credentials=creds, project_id=project_id)\n",
    "\n",
    "prompt_to_question= LLMChain(llm=question_model, prompt=prompt_1)\n",
    "question_to_answer = LLMChain(llm=answer_model, prompt=prompt_2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_question, question_to_answer],verbose=True)\n",
    "\n",
    "qa.run(\"Indonesia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "\n",
    "Anda telah menyelesaikan Lab ini, terkadang ada beberapa prompt template yang harus anda coba sesuaikan agar mampu menghasilkan jawaban yang lebih baik lagi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
